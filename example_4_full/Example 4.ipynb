{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d6e5cac",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.9/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Let's get our dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9f424f8",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      " Here we can sneek a peak at the sales data for day 01\n",
      "   sales_date   quantity   revenue   user_id\n",
      "0  2022-01-01          1         5         1\n",
      "1  2022-01-01          2        10         2\n",
      "2  2022-01-01          4        20         3\n",
      "===\n",
      " And here's the user data\n",
      "   user_id   name     status\n",
      "0        1   sven    premium\n",
      "1        2   john   standard\n",
      "2        4    eve    premium\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# IGNORE THE CODE BLOCK BELOW IT MOCKS STUFF AND YOU'RE NOT SUPPOSED TO KNOW THAT :P\n",
    "# ...\n",
    "### THIS CODE IS are a replacement for EXTERNAL API'S. Assume this is actually an REST API or something like that...\n",
    "### that you might call to get data from...\n",
    "\n",
    "def get_source_A(day: str):\n",
    "    \"\"\"\n",
    "    I am a \"mostly\" stateless event driven API. I return events with an \"event_time\"...\n",
    "    \n",
    "    I return the \"sales\" data for a given day\n",
    "    \n",
    "    \"\"\"\n",
    "    return pd.read_csv(f\"source_data/sales_day_{day}.csv\")\n",
    "    \n",
    "def get_source_B():\n",
    "    \"\"\"\n",
    "    I am a mostly stateful API. I return the \"state\" as it is.\n",
    "    \n",
    "    I return the \"user\" data.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(f\"source_data/users_v{random.randint(1,2)}.csv\")\n",
    "\n",
    "\n",
    "print(\"===\\n Here we can sneek a peak at the sales data for day 01\")\n",
    "print(get_source_A(\"01\"))\n",
    "print(\"===\\n And here's the user data\")\n",
    "print(get_source_B())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067f335",
   "metadata": {},
   "source": [
    "**Step 1**: As in every data warehouse, the step 1 is to get the data into the system, into some kind of \"staging area\". Since we're building a fully functional data warehouse here, our \"staging area\" is going to be immutable, or persitent. \n",
    "\n",
    "To do so we:\n",
    "\n",
    "1. Call the source_A once a day, and put the results into a file called \"sales_A.csv\". We add a column for the \"event processing time\" to make sure we are able to replay the ingestion (which is just a simple select on that column). \n",
    "2. Call the source_B once a day, and snapshot the whole raw data into files \"users_dayXYZ.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d98e6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652269546.612016 staging_source_A/01_1652269546.612016_data.csv\n",
      "1652269481.905827 staging_source_B/1652269481.905827_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# Data Ingestion Code\n",
    "# API Data => Immutable Staging Area\n",
    "def stage_source_A(today: str) -> (str, str):\n",
    "    data = get_source_A(today)\n",
    "    dt = datetime.now()\n",
    "    event_processing_time = datetime.timestamp(dt)\n",
    "    data[\"event_proc_time\"] = event_processing_time\n",
    "    target_file = f'staging_source_A/{today}_{event_processing_time}_data.csv'\n",
    "    data.to_csv(target_file, index=False)\n",
    "    \n",
    "    return event_processing_time, target_file\n",
    "\n",
    "### this runs daily! \n",
    "stage_source_A(\"01\")    \n",
    "### But if we run it twice, the time at which we run it is an implicit input and recorded as well, so this:...\n",
    "ep_time_sales, sales_partition = stage_source_A(\"01\")   \n",
    "print(ep_time_sales, sales_partition)\n",
    "### Is still going to produce an immutable staging area!\n",
    "\n",
    "def stage_source_B(today: str)-> (str, str):\n",
    "    data = get_source_B()\n",
    "    dt = datetime.now()\n",
    "    event_processing_time = datetime.timestamp(dt)\n",
    "    data[\"event_proc_time\"] = event_processing_time\n",
    "    target_file = f'staging_source_B/{event_processing_time}_data.csv'\n",
    "    data.to_csv(target_file, index=False)\n",
    "    \n",
    "    return event_processing_time, target_file\n",
    "\n",
    "### this runs daily! But really, we don't care at all how often it runs, because it doesn't carry any timestamp \n",
    "### anyways...\n",
    "stage_source_B(\"01\")    \n",
    "stage_source_B(\"01\") \n",
    "ep_time_users_1, users_partition_1 = stage_source_B(\"01\") \n",
    "ep_time_users_2, users_partition_2 = stage_source_B(\"02\")    \n",
    "\n",
    "print(ep_time_users, users_partition)\n",
    "### Is still going to produce an immutable staging area!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ccd2c4",
   "metadata": {},
   "source": [
    "\n",
    "**Step 2**: As in every data warehouse, the next step is to build up a modelling area where we process our \n",
    "    data a bit. This is going to be minor here, we're ...\n",
    "    \n",
    "1. Going to remove the \"names\" from the raw user input, we don't need them.\n",
    "2. We're going to keep the partitioning based on event_time\n",
    "3. For the \"sales data\" we add a column article filled with \"article_1\". Right now we just have one article but \n",
    "we plan on adding more so that seems nice to have\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9f17213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1652269546.612016, 'model_sales/01_1652269546.612016.csv')\n",
      "(1652269546.649704, 'model_users/1652269546.649704.csv')\n"
     ]
    }
   ],
   "source": [
    "def model_sales(today: str, proc_time: str, partition: str) -> str:\n",
    "    # Load the right partition\n",
    "    data = pd.read_csv(partition)\n",
    "    \n",
    "    # Do some modelling\n",
    "    data[\"article\"] = \"article_1\"\n",
    "    target_file = f'model_sales/{today}_{proc_time}.csv'\n",
    "    \n",
    "    # Store new data\n",
    "    data.to_csv(target_file)\n",
    "    return proc_time, target_file\n",
    "\n",
    "# Let's do our modelling run for today on the partition we've loaded before\n",
    "model_sales(\"01\", ep_time_sales, sales_partition)\n",
    "\n",
    "# And yes, doing this a dozen times is fine because it's idempotent!\n",
    "model_sales(\"01\", ep_time_sales, sales_partition)\n",
    "model_ep_time_sales, model_sales_partition  = model_sales(\"01\", ep_time_sales, sales_partition)\n",
    "print(model_sales(\"01\", ep_time_sales, sales_partition))\n",
    "\n",
    "\n",
    "# And yes again, the \"today\" part is just a bit of convenience, no need to calculate with time stamps... \n",
    "\n",
    "def model_users(proc_time: str, partition: str) -> str:\n",
    "    # Load the right partition\n",
    "    data = pd.read_csv(partition, header=0)\n",
    "\n",
    "    # Do some modelling\n",
    "    data.drop(columns=[\"name\"], axis=0, inplace=True)\n",
    "    target_file = f'model_users/{proc_time}.csv'\n",
    "\n",
    "    # Store new data\n",
    "    data.to_csv(target_file)\n",
    "    return proc_time, target_file\n",
    "\n",
    "model_users(ep_time_users_1, users_partition_1)\n",
    "model_users(ep_time_users_2, users_partition_2)\n",
    "\n",
    "# Again, this is idempotent so we can do this again and again!!! No worries about dropping something here,\n",
    "# We can always restore it from the raw data and the partition....\n",
    "print(model_users(ep_time_users_2, users_partition_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9c78e",
   "metadata": {},
   "source": [
    "**Step 3**: This is going to be a fun step, now we want to provide the data to the external world, so we decide to aggregate the data into a new table agg_sales containing a join of the two models on the user_id. Here's what \n",
    "we're going to do:\n",
    "\n",
    "1. Join the two models on the user_id\n",
    "2. BUT we will write a small \"macro\" to get the latest version of our user data\n",
    "3. (We're not going to write one for the sales data, we're assuming we're just doing one daily run, be we could do multiple ones!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cea81db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  sales_date   quantity   revenue   user_id  event_proc_time  \\\n",
      "0           0  2022-01-01          1         5         1     1.652270e+09   \n",
      "1           1  2022-01-01          2        10         2     1.652270e+09   \n",
      "2           2  2022-01-01          4        20         3     1.652270e+09   \n",
      "\n",
      "     article  user_id     status  \n",
      "0  article_1        1   standard  \n",
      "1  article_1        2   standard  \n",
      "2  article_1        3    premium  \n"
     ]
    }
   ],
   "source": [
    "# Sales data\n",
    "today = \"01\"\n",
    "\n",
    "def latest_user_partition():\n",
    "    \"\"\"\n",
    "    Returns the filepath of the latest user partition...\n",
    "    \n",
    "    \"\"\"\n",
    "    from os import listdir\n",
    "    user_partitions = listdir(\"./model_users\")\n",
    "    user_partitions.sort()\n",
    "    user_partitions.pop(0)\n",
    "    return f\"model_users/{user_partitions[-1]}\"\n",
    "\n",
    "latest_user_partition = latest_user_partition()\n",
    "\n",
    "sales_data = pd.read_csv(model_sales_partition, header=0)\n",
    "users_data = pd.read_csv(latest_user_partition, header=0)\n",
    "\n",
    "agg_sales = pd.merge(sales_data, users_data[[\"user_id\",\"status\"]], left_on=' user_id', right_on='user_id')\n",
    "\n",
    "target_file = f'agg_sales/{today}_{proc_time}.csv'\n",
    "\n",
    "agg_sales.to_csv(target_file)\n",
    "\n",
    "print(agg_sales)\n",
    "\n",
    "### Again, this is idempotent! Run it again, and only one file is there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16007647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
