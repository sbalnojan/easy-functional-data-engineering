{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b366996",
   "metadata": {},
   "source": [
    "# Easy Functional Data Engineering Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e358846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.9/site-packages (from pandas) (1.22.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "# Let's get our dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# some constants\n",
    "DATA_FILE_PATH=\"data.csv\"\n",
    "CONN = sqlite3.connect(':memory:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a0484bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our dataframe...\n",
      "    A  B  C\n",
      "0  1  2  a\n",
      "1  3  4  b\n",
      "2  5  6  a\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_FILE_PATH)\n",
    "print(\"This is our dataframe...\\n\", df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3261815",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c613f",
   "metadata": {},
   "source": [
    "**Data Engineering Task:** Alright, the task is simple... DATA_FILE_PATH is our external data source, and we want to load the data into our analytical database CONN. While doing that, it'd be cool to turn the \"a\"s and \"b\"s into proper names \"analytical_data\" and \"business_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb72deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a simple function to do that...\n",
    "\n",
    "def get_n_write_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df['C'] = df['C'].apply(lambda x: \"analytical_data\" if x == \"a\" else \"business_data\")\n",
    "    rows_written = df.to_sql(\"our_data_table\",CONN, if_exists=\"append\")\n",
    "    print(f\"Written {rows_written} rows into the db\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8908849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 3 rows into the db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_write_data(DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c125b2d",
   "metadata": {},
   "source": [
    "### Let's see how this is not functional data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218d205",
   "metadata": {},
   "source": [
    "Note: I'm gonna say \"function\" but really, the right unit to use is more like \"task\", or simply \"unit\" which might incoporate more than just a function we're calling. It might be a Sparkjob + some files, it might be a dbt run, it might be a lot of things.\n",
    "\n",
    "Alright, so our functions should be \"pure\", \"immutable\" and idempotent so how does that work out right now?\n",
    "\n",
    "1. Idempotency, what happens if we call it twice?\n",
    "=> We duplicate the data. Not cool.\n",
    "\n",
    "=> The easy fix for this dataset? We could just change to \"replace\" table.\n",
    "\n",
    "2. Immutable things. So what happens, if the underlying datafile changes? \n",
    "=> We get a different outcome, which might be intended, or it might not. In any case, we have some mutability inside here. \n",
    "\n",
    "3. Pureness. What are the sideeffects of the function?\n",
    "=> The function both is influenced by external conditions (namely the contents of the \"path\") as well as having an influence on external things (the database table).\n",
    "\n",
    "=> Let's see what we could do about that... \n",
    "=> There's no way around having sideeffects, if we're dealing with reading inputs & outputs, but there is a way to minimize them! It's to use wrappers just around the I/O part!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa262c",
   "metadata": {},
   "source": [
    "#### 1.1 Making parts of the task functionally pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39293634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(input_dataframe):\n",
    "    input_dataframe['C'] = input_dataframe['C'].apply(lambda x: \"analytical_data\" if x == \"a\" else \"business_data\")\n",
    "    return input_dataframe\n",
    "    \n",
    "def get_n_write_data(path):\n",
    "    df = pd.read_csv(path)    \n",
    "    output_dataframe = transform_data(df)\n",
    "    rows_written = df.to_sql(\"our_data_table\",CONN, if_exists=\"append\")\n",
    "    print(f\"Written {rows_written} rows into the db\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5aef34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 3 rows into the db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_write_data(DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d8a841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The benefit? The transformation behavior just became reproducible and testable! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a3ab69",
   "metadata": {},
   "source": [
    "#### 1.2 Making the task idempotent\n",
    "To make the task idempotent, we need to change the \"appending\" behavior. In this case, that's pretty simple, we stop to append, and simply always replace everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6991a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(input_dataframe):\n",
    "    input_dataframe['C'] = input_dataframe['C'].apply(lambda x: \"analytical_data\" if x == \"a\" else \"business_data\")\n",
    "    return input_dataframe\n",
    "    \n",
    "def get_n_write_data(path):\n",
    "    df = pd.read_csv(path)    \n",
    "    output_dataframe = transform_data(df)\n",
    "    rows_written = df.to_sql(\"our_data_table\",CONN, if_exists=\"replace\") #easy change in this case.\n",
    "    print(f\"Written {rows_written} rows into the db\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131f9ae",
   "metadata": {},
   "source": [
    "#### 1.3 Making the task more immutable\n",
    "Finally we can do one thing, to make our task a bit more immutable. What happens if the file changes? Well, everything goes down the drain. So what could we do? We could simply copy it to a unique place before running our stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3eed837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the current date and time\n",
    "\n",
    "# THIS FUNCTION IS UNITTESTABLE AND PURE\n",
    "def transform_data(input_dataframe):\n",
    "    input_dataframe['C'] = input_dataframe['C'].apply(lambda x: \"analytical_data\" if x == \"a\" else \"business_data\")\n",
    "    return input_dataframe\n",
    "\n",
    "\n",
    "# THIS FUNCTION IS UNITTESTABLE AND PURE\n",
    "def copy_to_unique(path):\n",
    "    # generate unique timestamp for EVENTPROCESSING TIME\n",
    "    dt = datetime.now()\n",
    "    ts = datetime.timestamp(dt)\n",
    "    target_file = f'{ts}_data.csv'\n",
    "    shutil.copyfile(path, target_file)\n",
    "    return target_file\n",
    "\n",
    "# THIS FUNCTION IS UNITTESTABLE AND PURE \n",
    "def write_data(staging_file):\n",
    "    df = pd.read_csv(staging_file)  \n",
    "    output_dataframe = transform_data(df)\n",
    "    rows_written = df.to_sql(\"our_data_table\",CONN, if_exists=\"replace\") #easy change in this case.\n",
    "    print(f\"Written {rows_written} rows into the db\")    \n",
    "\n",
    "# THE ONLY SMALL NON-PURE PART    \n",
    "def get_n_write_data(path):\n",
    "    staging_file = copy_to_unique(path)\n",
    "    write_data(staging_file)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb1e3cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 3 rows into the db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_write_data(DATA_FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
